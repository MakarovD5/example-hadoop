# 大数据

## 当前观看集数：47

## 简介

星号（*）代表我自己的注释

## 环境配置

### 基础Linux配置

- 关防火墙

- host配置，主机名设置

- ssh免密

- 集群时间同步

  ntpdate ntp4.aliyun.com

### Java配置

- 创建统一的工作目录

- 解压jdk包

- 配置环境变量

  记得之后更新下配置文件

  ```shell
  source /etc/profile
  ```

- 查看java安装路径

  ```shell
  which java
  ```

  

- 用scp复制到别的服务器，jdk和环境配置都复制，结合FinalShell全部会话命令

  > ```shell
  > # 主机名 
  > cat /etc/hostname
  > 
  > # hosts映射
  > vim /etc/hosts
  > 
  > 127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
  > ::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
  > 
  > 192.168.88.151 node1.itcast.cn node1
  > 192.168.88.152 node2.itcast.cn node2
  > 192.168.88.153 node3.itcast.cn node3
  > 
  > # JDK 1.8安装  上传 jdk-8u241-linux-x64.tar.gz到/export/server/目录下
  > cd /export/server/
  > tar zxvf jdk-8u241-linux-x64.tar.gz
  > 
  > 	#配置环境变量
  > 	vim /etc/profile
  > 	
  > 	export JAVA_HOME=/export/server/jdk1.8.0_241
  > 	export PATH=$PATH:$JAVA_HOME/bin
  > 	export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
  > 	
  > 	#重新加载环境变量文件
  > 	source /etc/profile
  > 
  > # 集群时间同步
  > ntpdate ntp5.aliyun.com
  > 
  > # 防火墙关闭
  > firewall-cmd --state	#查看防火墙状态
  > systemctl stop firewalld.service  #停止firewalld服务
  > systemctl disable firewalld.service  #开机禁用firewalld服务
  > 
  > # ssh免密登录（只需要配置node1至node1、node2、node3即可）
  > 
  > 	#node1生成公钥私钥 (一路回车)
  > 	ssh-keygen  
  > 	
  > 	#node1配置免密登录到node1 node2 node3
  > 	ssh-copy-id node1
  > 	ssh-copy-id node2
  > 	ssh-copy-id node3
  > ```

### Hadoop配置

- 解压编译好的安装包（正常情况需要自己从源码编译或者官方编译后的）

  为什么要编译：hadoop官方自己编译好的程序在链接动态链接库时容易出现问题，需要自己编译符合自己机器的程序

- 配置文件：/export/server/hadoop-3.3.0/etc/hadoop

  第一类1个：hadoop-env.sh

  第二类4个：xxxx-site.xml ,site表示的是用户定义的配置，会覆盖default中的默认配置。

  ​						core-site.xml 核心模块配置

  ​						hdfs-site.xml hdfs文件系统模块配置

  ​						mapred-site.xml MapReduce模块配置

  ​						yarn-site.xml yarn模块配置

   第三类1个：workers

  > 默认配置文件内容：
  >
  > https://hadoop.apache.org/docs/r3.3.1/hadoop-project-dist/hadoop-common/core-default.xml

- 将Hadoop添加到环境变量

- scp分发同步Hadoop安装包和环境变量profile文件

  ```shell
  scp -r hadoop-3.3.0 root@node2:$PWD
  ```

- 记得之后更新下配置文件

  ```shell
  source /etc/profile
  ```

- 首次启动HDFS时，必须对其进行格式化操作。首次启动要格式化namenode，format只能进行一次 后续不再需要，只在第一台机器

  ```shell
  hdfs namenode -format
  ```

  

  > hadoop-env.sh
  >
  > ```shell
  > #文件最后添加
  > export JAVA_HOME=/export/server/jdk1.8.0_241
  > 
  > export HDFS_NAMENODE_USER=root
  > export HDFS_DATANODE_USER=root
  > export HDFS_SECONDARYNAMENODE_USER=root
  > export YARN_RESOURCEMANAGER_USER=root
  > export YARN_NODEMANAGER_USER=root 
  > ```
  >
  > core-site.xml
  >
  > ```xml
  > <!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 -->
  > <property>
  >     <name>fs.defaultFS</name>
  >     <value>hdfs://node1:8020</value>
  > </property>
  > 
  > <!-- 设置Hadoop本地保存数据路径 -->
  > <property>
  >     <name>hadoop.tmp.dir</name>
  >     <value>/export/data/hadoop-3.3.0</value>
  > </property>
  > 
  > <!-- 设置HDFS web UI用户身份 -->
  > <property>
  >     <name>hadoop.http.staticuser.user</name>
  >     <value>root</value>
  > </property>
  > 
  > <!-- 整合hive 用户代理设置 -->
  > <property>
  >     <name>hadoop.proxyuser.root.hosts</name>
  >     <value>*</value>
  > </property>
  > 
  > <property>
  >     <name>hadoop.proxyuser.root.groups</name>
  >     <value>*</value>
  > </property>
  > 
  > <!-- 文件系统垃圾桶保存时间 -->
  > <property>
  >     <name>fs.trash.interval</name>
  >     <value>1440</value>
  > </property>
  > ```
  >
  > hdfs-site.xml
  >
  > ```xml
  > <!-- 设置SNN进程运行机器位置信息 -->
  > <property>
  >     <name>dfs.namenode.secondary.http-address</name>
  >     <value>node2:9868</value>
  > </property>
  > ```
  >
  > mapred-site.xml
  >
  > ```xml
  > <!-- 设置MR程序默认运行模式： yarn集群模式 local本地模式 -->
  > <property>
  >   <name>mapreduce.framework.name</name>
  >   <value>yarn</value>
  > </property>
  > 
  > <!-- MR程序历史服务地址 -->
  > <property>
  >   <name>mapreduce.jobhistory.address</name>
  >   <value>node1:10020</value>
  > </property>
  >  
  > <!-- MR程序历史服务器web端地址 -->
  > <property>
  >   <name>mapreduce.jobhistory.webapp.address</name>
  >   <value>node1:19888</value>
  > </property>
  > 
  > <property>
  >   <name>yarn.app.mapreduce.am.env</name>
  >   <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
  > </property>
  > 
  > <property>
  >   <name>mapreduce.map.env</name>
  >   <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
  > </property>
  > 
  > <property>
  >   <name>mapreduce.reduce.env</name>
  >   <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
  > </property>
  > ```
  >
  > yarn-site.xml
  >
  > ```xml
  > <!-- 设置YARN集群主角色运行机器位置 -->
  > <property>
  > 	<name>yarn.resourcemanager.hostname</name>
  > 	<value>node1</value>
  > </property>
  > 
  > <property>
  >     <name>yarn.nodemanager.aux-services</name>
  >     <value>mapreduce_shuffle</value>
  > </property>
  > 
  > <!-- 是否将对容器实施物理内存限制 -->
  > <property>
  >     <name>yarn.nodemanager.pmem-check-enabled</name>
  >     <value>false</value>
  > </property>
  > 
  > <!-- 是否将对容器实施虚拟内存限制。 -->
  > <property>
  >     <name>yarn.nodemanager.vmem-check-enabled</name>
  >     <value>false</value>
  > </property>
  > 
  > <!-- 开启日志聚集 -->
  > <property>
  >   <name>yarn.log-aggregation-enable</name>
  >   <value>true</value>
  > </property>
  > 
  > <!-- 设置yarn历史服务器地址 -->
  > <property>
  >     <name>yarn.log.server.url</name>
  >     <value>http://node1:19888/jobhistory/logs</value>
  > </property>
  > 
  > <!-- 历史日志保存的时间 7天 -->
  > <property>
  >   <name>yarn.log-aggregation.retain-seconds</name>
  >   <value>604800</value>
  > </property>
  > ```
  >
  > workers
  >
  > ```
  > node1.itcast.cn
  > node2.itcast.cn
  > node3.itcast.cn
  > ```
  >
  > 将hadoop添加到环境变量（3台机器）
  >
  > ```shell
  > vim /etc/profile
  > 
  > export HADOOP_HOME=/export/server/hadoop-3.3.0
  > export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
  > 
  > source /etc/profile
  > 
  > 
  > #别忘了scp给其他两台机器哦
  > ```

## Hadoop

### 最初使用

#### shell脚本一键启停

前提：配置好机器之间的SSH免密登录和workers文件。

HDFS集群：
start-dfs.sh 
stop-dfs.sh 
YARN集群：
start-yarn.sh
stop-yarn.sh
Hadoop集群：
start-all.sh
stop-all.sh 

#### 启动出现问题记得看日志

/export/server/hadoop-3.3.0/logs/

#### Web UI

HDFS集群：http://namenode_host:9870

YARN集群：http://resourcemanager_host:8088

### 重要特性

- 主从架构

  一主多从

- 分块存储

  默认128M，从物理上将文件分割，存储由datanode完成

- 副本机制

  默认有2个副本，即每个块在3个位置分别存储

- 元数据记录

  由namenode完成，记录文件信息和块的储存位置

- 抽象统一的目录树结构（namespace）

  层次型文件组织结构，即目录树

### 模拟实现分布式文件存储

1.如何解决海量数据存的下的--**分布式存储**

2.如何解决海量数据文件查询便捷—-**元数据记录**

3.如何解决大文件传输效率慢—-**分块存储**

4.如何解决硬件故障数据丢失--**副本机制**

5.如何解决用户查询视角统一规整--**抽象目录树结构**

## HDFS

### HDFS设计目标

\1.   HDFS集群由很多的服务器组成，而每一个机器都与可能会出现故障。HDFS为了能够进行故障检测、快速恢复等。

\2.   HDFS主要适合去做批量数据出来，相对于数据请求时的反应时间，HDFS更倾向于保障吞吐量。（*数据处理往往一周一次、一月一次等等，不需要考虑访问当时的反应速度）

\3.   典型的HDFS中的文件大小是GB到TB，HDFS比较适合存储大文件

\4.   HDFS很多时候是以： Write-One-Read-Many来应用的，一旦在HDFS创建一个文件，写入完后就不需要修改了。（*比如记录昨天的天气以后一定不会再修改了）

\5.   移动计算的代价比之移动数据的代价低。一个应用请求的计算，离它操作的数据越近就越高效。将计算移动到数据附近，比之将数据移动到应用所在显然更好。

### Shell

hadoop fs [generic options]

#### 文件系统协议

hadoop fs -ls file:/// #操作本地文件系统
hadoop fs -ls hdfs://node1:8020/ #操作HDFS分布式文件系统
hadoop fs -ls / #直接根目录，没有指定协议 将加载读取**fs.defaultFS**值

hadoop dfs 只能操作HDFS文件系统（包括与Local FS间的操作），不过已经Deprecated；
hdfs dfs 只能操作HDFS文件系统相关（包括与Local FS间的操作）,常用；
hadoop fs 可操作任意文件系统，不仅仅是hdfs文件系统，使用范围更广；
目前版本来看，官方最终推荐使用的是hadoop fs。当然hdfs dfs在市面上的使用也比较多。
可以通过hadoop fs -help命令来查看每个命令的详细用法。(*看完help就相当于看完所有的shell命令，极大提高功力)

#### 常用命令

##### 创建文件夹

hadoop fs -mkdir [-p] <path> ... 
path 为待创建的目录
-p选项的行为与Unix mkdir -p非常相似，它会沿着路径创建父目录。

```shell
hadoop fs -mkdir /itcast
```

##### 查看指定目录下内容

hadoop fs -ls [-h] [-R] [<path> ...] 
path 指定目录路径
-h 人性化显示文件size
-R 递归查看指定目录及其子目录

##### 上传文件到HDFS指定目录下

 hadoop fs -put [-f] [-p] <localsrc> ... <dst>
-f 覆盖目标文件（已存在下）
-p 保留访问和修改时间，所有权和权限。
localsrc 本地文件系统（**客户端所在机器**）
dst 目标文件系统（HDFS）

```shell
hadoop fs -put zookeeper.out /itcast
hadoop fs -put file:///etc/profile hdfs://node1:8020/itcast
```

**moveFromLocal**
和put参数类似，但是源文件localsrc拷贝之后自身被删除
hdfs dfs **-**moveFromLocal **<**localsrc**>** **<**dst**>**

##### 查看HDFS文件内容

hadoop fs -cat <src> ... 
读取指定文件全部内容，显示在标准输出控制台。
注意：对于大文件内容读取，慎重。（*可用head和tail命令）

```shell
hadoop fs -cat /itcast/zookeeper.out
```

**head 命令**

hdfs dfs -head URI

**tail 命令**

hdfs dfs -tail [-f] URI

-f选项表示数据只要有变化也会输出到控制台。

##### 下载HDFS文件

hadoop fs -get [-f] [-p] <src> ... <localdst>
下载文件到本地文件系统指定目录，localdst必须是目录
-f 覆盖目标文件（已存在下）
-p 保留访问和修改时间，所有权和权限。

```shell
[root@node2 ~]# mkdir test
[root@node2 ~]# cd test/
[root@node2 test]# ll
total 0
[root@node2 test]# hadoop fs -get /itcast/zookeeper.out ./
（*./代表当前路径）
[root@node2 test]# ll
total 20
-rw-r--r-- 1 root root 18213 Aug 18 17:54 zookeeper.out
```

**合并下载getmerge**

hadoop fs -getmerge [-nl] [-skip-empty-file] <src> <localdst> 
	下载多个文件合并到本地文件系统的一个文件中。
	-nl选项表示在每个文件末尾添加换行符
	-skip-empty-file跳过空文件

##### 拷贝HDFS文件

hadoop fs -cp [-f] <src> ... <dst> 
-f 覆盖目标文件（已存在下）

```shell
[root@node3 ~]# hadoop fs -cp /small/1.txt /itcast
[root@node3 ~]# hadoop fs -cp /small/1.txt /itcast/666.txt #重命令
[root@node3 ~]# hadoop fs -ls /itcast
Found 4 items
-rw-r--r-- 3 root supergroup 2 2021-08-18 17:58 /itcast/1.txt
-rw-r--r-- 3 root supergroup 2 2021-08-18 17:59 /itcast/666.txt
```

##### 追加数据到HDFS文件中

hadoop fs -appendToFile <localsrc> ... <dst>
将所有给定本地文件的内容追加到给定dst文件。
dst如果文件不存在，将创建该文件。
如果<localSrc>为-，则输入为从标准输入中读取。

```shell
#追加内容到文件尾部 appendToFile
[root@node3 ~]# echo 1 >> 1.txt
[root@node3 ~]# echo 2 >> 2.txt 
[root@node3 ~]# echo 3 >> 3.txt 
[root@node3 ~]# hadoop fs -appendToFile *.txt /1.txt
（*若1.txt有内容则追加到其后面，没有这个文件就新建这个文件）
[root@node3 ~]# hadoop fs -cat /1.txt
1
2
3
```

##### HDFS数据移动操作

hadoop fs -mv <src> ... <dst>
移动文件到指定文件夹下
可以使用该命令移动数据，重命名文件的名称

##### 查看HDFS磁盘使用情况

hdfs dfs -df [-h] URI [URI ...]

```shell
[root@node1 ~]# hdfs dfs -df -h /
Filesystem                      Size   Used  Available  Use%
hdfs://node1.itcast.cn:9820  346.6 G  2.1 G    236.7 G    1%
```

##### 显示目录中所有文件大小

**du 命令**

显示目录中所有文件大小，当只指定一个文件时，显示此文件的大小。

语法格式：
hdfs dfs -du [-s] [-h] [-v] [-x]  URI [URI ...]  

命令选项：
-s：表示显示文件长度的汇总摘要，而不是单个文件的摘要。
-h：选项将以“人类可读”的方式格式化文件大小
-v：选项将列名显示为标题行。
-x：选项将从结果计算中排除快照。 

```shell
[root@node1 ~]# hdfs dfs -du -s -h -v /source/weibo/
SIZE    DISK_SPACE_CONSUMED_WITH_ALL_REPLICAS  FULL_PATH_NAME
64.2 M  192.6 M                                /source/weibo
```

##### 修改HDFS文件副本个数

hadoop fs -setrep [-R] [-w] <rep> <path> ...
	修改指定文件的副本个数。
	-R表示递归 修改文件夹下及其所有
	-w 客户端是否等待副本修改完毕。
	<rep>副本数

（*最好提前规划好副本数，不然后面在进行这个setrep操作会很浪费时间资源）

```shell
hadoop fs -setrep -w 2 /tmp/caixukun_dirtydata.csv
```



##### 命令官方指导文档

[Apache Hadoop 3.3.0 – Overview](https://hadoop.apache.org/docs/r3.3.0/hadoop-project-dist/hadoop-common/FileSystemShell.html)

### HDFS Java 客户端 API

#### 客户端核心类

Configuration：该类的对象封转了客户端或者服务器的配置

FileSystem：该类的对象是一个文件系统对象，可以用该对象的一些方法来对文件进行操作，通过FileSystem的静态方法get获得该对象。

```java
FileSystem fs = FileSystem.get(conf)
```

- get方法从conf中的一个参数 fs.defaultFS的配置值判断具体是什么类型的文件系统。如果我们的代码中没有指定fs.defaultFS，并且工程classpath下也没有给定相应的配置，conf中的默认值就来自于hadoop的jar包中的core-default.xml，默认值为： file:///，则获取的将不是一个DistributedFileSystem的实例，而是一个本地文件系统的客户端对象。

#### 配置Maven

配置pom.xml

配置阿里源

maven-compiler-plugin插件报错：
安装maven仓库里有该插件的maven版本，安装后一定要**重启idea**

（*需不需要重启idea我自己总结一条规律：是不是自行懂了）

#### 连接HDFS

用单元测试的方式连接，添加@Before和@After

##### 修改默认用户

默认以windwos的用户登录，需要修改为root

```java
//设置客户端身份 以具备权限在HDFS操作
System.setProperty("HADOOP_USER_NAME","root");
```

##### 创建文件夹

mkdir()

##### 上传文件

copyFromLocalFile()

##### 下载文件

copyToLocalFile()

**下载报错**

错误提示：
	找不到winutils.exe、HADOOP_HOME没有设置
原因：
	Hadoop访问windows本地文件系统，要求Windows上的本地库能正常工作。
	其中Hadoop使用某些Windows API来实现类似posix的文件访问权限。
	上述功能需要在hadoop.dll和winutils.exe来实现。
解决：
	下载Hadoop源码在windows平台编译，编译出windows本地库。然后配置Hadoop环境变量。
HADOOP_HOME=C:\soft\hadoop-3.1.4
path=;%HADOOP_HOME%\bin

#### Log4J

##### Log4j具有三个主要组件

- Logger(日志记录器)
  Logger控制日志的输出级别与日志是否输出；
- Appender（输出端）
  Appender指定日志的输出方式（ConsoleAppender控制台、FileAppender文件、JDBCAppender等）
- Layout（日志格式化器）
  Layout控制日志信息的输出格式（simple格式、HTML格式、PatternLayout自定义格式）

##### 日志级别

Log4J 在 org.apache.log4j.Level 类中定义了OFF、FATAL、ERROR、WARN、INFO、DEBUG、TRACE、ALL八种日志级别

ERROR > WARN > INFO > DEBUG

| 目录  | 说明                                           |
| ----- | ---------------------------------------------- |
| ERROR | 发生错误事件，但仍不影响系统的继续运行         |
| WARN  | 警告，即潜在的错误情形                         |
| INFO  | 一般在粗粒度级别上，强调应用程序的运行全程     |
| DEBUG | 一般用于细粒度级别上，对调试应用程序非常有帮助 |

##### 程序中使用Log4j

- 项目中引入log4j的jar包
- 添加配置文件log4j.properties
- 代码中使用

###### 项目中引入log4j的jar包

hadoop自带log4j，可以不用自己再安装，在依赖项的hadoop-common里

###### 配置文件log4j.properties

可以在Hadoop安装路径里拿取：/export/server/hadoop-3.3.0/etc/hadoop/

```properties
#ConsoleAppender表示控制台，
log4j.appender.Console=org.apache.log4j.ConsoleAppender
#自定义格式
log4j.appender.Console.layout=org.apache.log4j.PatternLayout
#格式形式为时间，进程等等。。。
log4j.appender.Console.layout.ConversionPattern=%d [%t] %p [%c] - %m%n
#DEBUG表示级别，只显示DEBUG及以上的级别，如果为info，则debug及以下级别的不会输出。warn等等同理
#Consle只是个名字，起其他任何名字都行
log4j.rootLogger=DEBUG,Console
```

ConversionPattern的格式：

%p: 输出日志信息优先级，即DEBUG，INFO，WARN，ERROR，FATAL, 
%d: 输出日志时间点的日期或时间，默认格式为ISO8601，也可以在其后指定格式，比如：%d{yyyy-MM-dd HH:mm:ss,SSS}，输出类似：2011-10-18 22:10:28,921 
 %r: 输出自应用启动到输出该log信息耗费的毫秒数 
 %c: 输出日志信息所属的类目，通常就是所在类的全名 
 %t: 输出产生该日志事件的线程名 
 %l: 输出日志事件的发生位置，相当于%C.%M(%F:%L)的组合,包括类目名、发生的线程，以及在代码中的行数。 
 %x: 输出和当前线程相关联的NDC(嵌套诊断环境),尤其用到像java servlets这样的多客户多线程的应用中。 
 %%: 输出一个"%"字符 
 %F: 输出日志消息产生时所在的文件名称 
 %L: 输出代码中的行号 
 %m: 输出代码中指定的消息,产生的日志具体信息 
 %n: 输出一个回车换行符，Windows平台为"\r\n"，Unix平台为"\n"输出日志信息换行 

###### 代码中使用

```java
Logger logger = Logger.getLogger(对哪个类的记录的类的类名.class)
```

#### Google-option

创建实体类，创建命令行对应public字段，每个字段加@Option

```java
public class ServerOptions extends OptionsBase {
  @Option(
      name = "help",
      abbrev = 'h',
      help = "Prints usage info.",
      defaultValue = "true"
    )
  public boolean help;
}
```

使用：

```java
    OptionsParser parser = OptionsParser.newOptionsParser(ServerOptions.class);
    parser.parseAndExitUponError(args);
    ServerOptions options = parser.getOptions(ServerOptions.class);
```

#### 舆情数据上报案例

##### 环境准备

Google-option

Log4J

FileSystem类的使用，非常频繁

##### 实现生成数据采集任务

实现步骤：
	1.判断原始数据目录是否存在
	2.读取原始数据目录下的所有文件
	3.判断待上传目录是否存在，不存在则创建一个
	4.创建任务目录（目录名称：task_年月日时分秒_任务状态）
	5.遍历待上传的文件，在待上传目录生成一个willDoing文件
	6.将待移动的文件添加到willDoing文件中

知识点：

1、try-catch的catch部分改为用log4j输出到控制台：**（*为什么要再抛一个新的异常目前不清楚，可能为打印出具体错误行数栈）**

**在catch子句中可以抛出一个异常， 这样做的目的是 改变异常类型；强烈建议使用这种包装技术， 这样可以让用户抛出子系统中的高级异常， 而不会丢失原始异常的小细节；**

```java
Logger.error(e.getMessage(), e);
throw new RuntimeException(e.getMessage());
```

2、lambda函数：在文章尾部其他领域java部分

3、如果路径不存在，新建路径

```java
FileUtils.forceMkdirParent(tempDir);
```

4、FileUtils的使用

##### 实现执行数据上报任务

实现步骤：
1.	读取待上传目录的willDoing任务文件，注意过滤COPY和DONE后的任务文件夹
2.	遍历读取任务文件，开始上传
a)	将任务文件修改为COPY，表示正在处理中
b)	获取任务的日期
c)	判断HDFS目标上传目录是否存在，不存在则创建
d)	读取任务文件
e)	按照换行符切分
f)	上传每一个文件,调用HDFSUtils进行数据文件上传
g)	上传成功后，将COPY后缀修改为_DONE

(*思路总结，source是从网上爬的数据，按任务分类整理好挪到pending下，再统一上传到hdfs上)

### Hadoop基准测试

#### 写入基准测试

```shell
hadoop jar /export/server/hadoop-3.1.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.4-tests.jar  TestDFSIO -write -nrFiles 10  -fileSize 10MB
```

#### 读取基准测试

```shell
hadoop jar /export/server/hadoop-3.1.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.4-tests.jar  TestDFSIO -read -nrFiles 10 -fileSize 10MB
```

#### 清除测试数据

```shell
hadoop jar /export/server/hadoop-3.1.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.4-tests.jar   TestDFSIO -clean
```

#### 测试结果各项数值的意义

- `Number of files`：生成 mapTask 数量，一般是集群中 CPU 核数 -1，我们测试虚拟机就按照实际的物理内存 -1 分配即可
- `Total MBytes processed`：单个 map 处理的文件大小
- `Throughput mb/sec`：单个 mapTak 的吞吐量

计算方式：处理的总文件大小 / 每一个 mapTask 写数据的时间累加
 集群整体吞吐量：生成 mapTask 数量 * 单个 mapTak 的吞吐量

- `Average IO rate mb/sec`：平均 mapTak 的吞吐量

计算方式：每个 mapTask 处理文件大小 / 每一个 mapTask 写数据的时间全部相加除以 task 数量

- `IO rate std deviation`：方差、反映各个 mapTask 处理的差值，越小越均衡

### HDFS工作流程与机制

#### HDFS集群角色与职责

##### NameNode

- NameNode成为了访问HDFS的唯一入口。
- NameNode仅存储HDFS的元数据：文件系统中所有文件的目录树，并跟踪整个集群中的文件，不存储实际数据。
- NameNode知道HDFS中任何给定文件的块列表及其位置。使用此信息NameNode知道如何从块中构建文件。
- NameNode不持久化存储每个文件中各个块所在的datanode的位置信息，这些信息会在系统启动时从DataNode重建。
- NameNode是Hadoop集群中的单点故障。
- NameNode所在机器通常会配置有大量内存（RAM）。

##### DataNode

- DataNode是Hadoop HDFS中的从角色，负责具体的数据块存储。决定了HDFS集群的整体数据存储能力。
- DataNode负责最终数据块block的存储。是集群的从角色，也称为Slave。
- DataNode启动时，会将自己注册到NameNode并汇报自己负责持有的块列表。
- 当某个DataNode关闭时，不会影响数据的可用性。 NameNode将安排由其他DataNode管理的块进行副本复制。
- DataNode所在机器通常配置有大量的硬盘空间，因为实际数据存储在DataNode中。

#####  SecondaryNameNode

- Secondary NameNode充当NameNode的辅助节点，但不能替代NameNode。
- 主要是帮助主角色进行元数据文件的合并动作。可以通俗的理解为主角色的“秘书”。

#### HDFS读写数据流程

*读写流程面试常考，面试前可以看源码解析了解流程

##### Pipeline管道

- pipeline是线性传输，顺序的沿着一个方向传输，这样能够充分利用每个机器的带宽，避免网络瓶颈和高延迟时 的连接，最小化推送所有数据的延时。

##### ACK应答响应

- ACK (Acknowledge character）即是确认字符，在数据通信中，接收方发给发送方的一种传输类控制字符。表示发来的数据已确认接收无误。
- 在HDFS pipeline管道传输数据的过程中，传输的反方向会进行ACK校验，确保数据传输安全。

##### 默认3副本存储策略

- 默认副本存储策略是由BlockPlacementPolicyDefault指定。
- 第一块副本：优先客户端本地，否则随机
  第二块副本：不同于第一块副本的不同机架。（*机架就是一个衣柜样的机柜）
  第三块副本：第二块副本相同机架不同机器。

## MapReduce

## 其他领域

### Java

#### lambda的使用

```java
// 读取原始数据目录下的所有文件
File[] allSourceDataFile = sourceDir.listFiles(f -> {
    // 判断文件格式是否以 weibo_data_ 开头
    String fileName = f.getName();
    if (fileName.startsWith("weibo_data_")) {
        return true;
    }
    return false;
});
```

这里的listFiles()括号里的内容可以理解为：

首先，listFiles的参数形式有三种：空，FileFilter接口，FilenameFilter接口

```java
//分别看看接口的样子
public interface FileFilter {
    boolean accept(File pathname);
}

public interface FilenameFilter {
    boolean accept(File dir, String name);
}

```

lambda函数实现了FileFilter接口
为什么是FileFilter而不是FilenameFilter：因为lambda只有一个参数f，而FilenameFilter需要两个参数。

FileFilter中只有一个抽象方法，返回的是Boolean型，所以lambda函数就实现了这个方法。

总的来看：lambda返回的是FileFilter接口的实例对象，只有一个f参数，且f为File类型参数，lambda的{}内实现了accept方法，该方法返回的是boolean型。

使用Lambda时，要记住的就两点：

1. Lambda返回的是接口的实例对象
2. 有没有参数、参数有多少个、需不需要有返回值、返回值的类型是什么---->**选择自己合适的函数式接口**
3. **（*Lambda只能实现函数式接口，即只有一个抽象方法的接口，用@FunctionalInterface标注函数式接口，所以如果接口有两个抽象方法就无法使用lambda）**

#### FileUtils的使用

 FileUtils 是 Apache Commons IO 的一部分

```xml
<dependency>
    <groupId>commons-io</groupId>
    <artifactId>commons-io</artifactId>
    <version>2.6</version>
</dependency>
```

##### FileUtils.moveFile

When the destination file is on another file system, do a "copy and delete".

#### 捕获异常+再次抛出异常与异常链

在catch子句中可以抛出一个异常， 这样做的目的是 改变异常类型；强烈建议使用这种包装技术， 这样可以让用户抛出子系统中的高级异常， 而不会丢失原始异常的小细节；

```java
} catch (IOException e) {
//            e.printStackTrace();
            logger.error(e.getMessage(),e);
            throw new RuntimeException(e);
        }
```

#### JAVA IO

##### 创建文件

1. new File(String path);
2. new File(File father, String chilend);
3. new File(String father, String child);

##### 获取文件信息

1. getName();
2. getAbsolutePath();
3. getParent();
4. length();//字节数，英文1个字节，汉字3个字节
5. exists();
6. isFile();
7. isDirectory();

##### 目录操作

1. delete();//删除文件和空文件夹，文件夹里有文件就不行，目录
2. mkdir();
3. mkdirs();//递归创建

### Linux

#### proc目录

/proc是一个位于内存中的伪文件系统(in-memory pseudo-file system)。该目录下保存的不是真正的文件和目录，而是一些“运行时”信息，如系统内存、磁盘io、设备挂载信息和硬件配置信息等。

lsmod命令就是cat /proc/modules命令的别名，lspci命令是cat /proc/pci命令的别名。

- /proc/loadavg 保存了系统负载的平均值，其前三列分别表示最近1分钟、5分钟及15分的平均负载。反映了当前系统的繁忙情况。
- /proc/meminfo 当前内存使用的统计信息，常由free命令使用；可以使用文件查看命令直接读取此文件，其内容显示为两列，前者为统计属性，后者为对应的值；
- /proc/diskstats 磁盘设备的磁盘I/O统计信息列表;
- /proc/net/dev 网络流入流出的统计信息，包括接收包的数量、发送包的数量，发送数据包时的错误和冲突情况等。
- /proc/cmdline 在启动时传递至内核的启动参数，通常由grub启动管理工具进行传递；
- /proc/devices 系统已经加载的所有块设备和字符设备的信息；
- /proc/mounts 系统中当前挂载的所有文件系统；
- /proc/partitions 块设备每个分区的主设备号（major）和次设备号（minor）等信息，同时包括每个分区所包含的块（block）数目；
- /proc/uptime 系统上次启动以来的运行时间；
- /proc/version 当前系统运行的内核版本号，在作者的Debian系统中，还会显示系统安装的gcc版本；
- /proc/vmstat 当前系统虚拟内存的统计数据。

#### scp命令：基于ssh远程拷贝

scp 是 secure copy 的缩写, scp 是 linux 系统下基于 ssh 登陆进行安全的远程文件拷贝命令。可以结合SSH免密，并设置host地址，把IP数字设置成简短的英文名

参数-r： 递归复制整个目录。复制目录时要加-r

```shell
scp -r jdk1.8.0_241 root@node2:/export/server/
```

#### FinalShell的命令编辑器能发送到全部会话

### IDEA

#### pom报错

如果手动修改了本地文件，pom报错，重启一下idea，因为idea无法动态识别

#### 快速选中一行

一、鼠标连续点三下
二、end键将光标移到行尾 ， ctrl+w 选中行
三、end键将光标移到行尾 ， shift + home 选中行
四、home 键 光标移到行首、然后 点击shift +end

### Maven

#### maven插件版本不需要和maven版本对应，每个maven插件都有自己的版本

### 个人成长

#### PEST分析法

从政治、经济、社会、技术因素去分析企业管理经营的问题。可以应用到对个人的分析

### 其他

#### 命令行英文缩写CLI

命令行界面（英语：command-line interface，缩写：CLI）

#### 几个磁盘架构知识

##### 磁盘阵列

RAID2、3、4较少实际应用，它们大多只在研究领域有实作。

**◇RAID 0**

- 优点：使用 n 颗硬盘，即可拥有将近 n 倍的读写效能。
- 缺点：数据安全性较低，同组数组中任一硬盘发生问题就会造成数据遗失。
- 硬盘数量：最少 2 个。

![img](https://pic1.zhimg.com/80/v2-29d439a5d1a24127bc48d41e42b093af_720w.jpg?source=1940ef5c)

**◇RAID 1**

- 优点：安全性依照数组里的实体硬盘数量倍数成长。
- 缺点：空间利用率是所有 RAID 中最没有效率的。
- 硬盘数量：最少 2 个。

![img](https://pic4.zhimg.com/80/v2-731c286299fee461a9e0c87ca231df16_720w.jpg?source=1940ef5c)

**◇RAID 5**

- 优点：兼顾空间利用率与安全性。
- 缺点：需要额外的运算资源，仅能忍受 1 个硬盘损毁。
- 硬盘数量：至少 3 个。

![img](https://pic4.zhimg.com/80/v2-2a1d0b4b5db928cd2b8df7f5c50f8455_720w.jpg?source=1940ef5c)

**◇RAID 6**

- 优点：容错硬盘数量比 RAID 5 多 1 颗。
- 缺点：运算量比 RAID 5 大、空间利用率比 RAID 5 低。
- 硬盘数量：至少 4 个。

![img](https://pic2.zhimg.com/80/v2-5187b267e31caac37e08d5a7f958f997_720w.jpg?source=1940ef5c)

##### DAS，NAS，SAN对比

![image-20220305153211208](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220305153211208.png)



